{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2464f8-72d2-491c-800d-73d5186aa227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8dadbd-b7d1-4b4b-9e2b-fb75b1c58c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees = 0, translate=(0.1, 0.1)),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomGrayscale(),\n",
    "    # transforms.RandomPerspective(),\n",
    "    transforms.ToTensor()]) #  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor()]) # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "train_dl = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=4,\n",
    "                                            prefetch_factor = 2, \n",
    "                                            pin_memory = True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "test_dl = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=4,\n",
    "                                             prefetch_factor = 2, \n",
    "                                            pin_memory = True)\n",
    "\n",
    "val_dl = test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d15115-cde4-49d7-ab21-e7eb293a7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577ed534-bc59-417d-a0d7-6dda76075a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "is_cuda=torch.cuda.is_available()\n",
    "device=torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "print(device)\n",
    "model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db17e97-7e7f-40c5-ab92-9d9df588a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-5, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96184bbd-97ac-4c64-a767-cd6bfe1f0c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 1.436\n",
      "Got 5849 / 10000 with accuracy 58.49\n",
      "Loss in epoch 1: 0.962\n",
      "Got 6772 / 10000 with accuracy 67.72\n",
      "Loss in epoch 2: 0.757\n",
      "Got 6388 / 10000 with accuracy 63.88\n",
      "Loss in epoch 3: 0.622\n",
      "Got 7746 / 10000 with accuracy 77.46\n",
      "Loss in epoch 4: 0.541\n",
      "Got 7924 / 10000 with accuracy 79.24\n",
      "Loss in epoch 5: 0.479\n",
      "Got 7640 / 10000 with accuracy 76.40\n",
      "Loss in epoch 6: 0.432\n",
      "Got 8340 / 10000 with accuracy 83.40\n",
      "Loss in epoch 7: 0.395\n",
      "Got 8294 / 10000 with accuracy 82.94\n",
      "Loss in epoch 8: 0.360\n",
      "Got 8261 / 10000 with accuracy 82.61\n",
      "Loss in epoch 9: 0.331\n",
      "Got 8547 / 10000 with accuracy 85.47\n",
      "Loss in epoch 10: 0.303\n",
      "Got 8748 / 10000 with accuracy 87.48\n",
      "Loss in epoch 11: 0.283\n",
      "Got 8872 / 10000 with accuracy 88.72\n",
      "Loss in epoch 12: 0.262\n",
      "Got 8854 / 10000 with accuracy 88.54\n",
      "Loss in epoch 13: 0.237\n",
      "Got 8781 / 10000 with accuracy 87.81\n",
      "Loss in epoch 14: 0.226\n",
      "Got 8780 / 10000 with accuracy 87.80\n",
      "Loss in epoch 15: 0.210\n",
      "Got 8716 / 10000 with accuracy 87.16\n",
      "Loss in epoch 16: 0.200\n",
      "Got 8878 / 10000 with accuracy 88.78\n",
      "Loss in epoch 17: 0.186\n",
      "Got 8878 / 10000 with accuracy 88.78\n",
      "Loss in epoch 18: 0.173\n",
      "Got 8907 / 10000 with accuracy 89.07\n",
      "Loss in epoch 19: 0.166\n",
      "Got 9054 / 10000 with accuracy 90.54\n"
     ]
    }
   ],
   "source": [
    "oss_store_train =[]\n",
    "loss_store_val = []\n",
    "acc_store=[]\n",
    "epoches=20\n",
    "xaxis=np.arange(0,epoches)\n",
    "\n",
    "for epoch in range(epoches): #I decided to train the model for 50 epochs\n",
    "    loss_ep = 0\n",
    "    model.train()\n",
    "    for _, (data, targets) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        ## Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        score = model(data)\n",
    "        loss = criterion(score,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        loss_ep += loss.item()\n",
    "    print(f\"Loss in epoch {epoch}: {loss_ep/len(train_dl):.3f}\")\n",
    "    loss_store_train.append(loss_ep/len(train_dl))\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        for _, (data,targets) in enumerate(val_dl):\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            ## Forward Pass\n",
    "            score = model(data)\n",
    "            _, predictions = score.max(1)\n",
    "            loss2 = criterion(score,targets)\n",
    "            valid_loss += loss2.item()\n",
    "            \n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}\"\n",
    "        )\n",
    "        acc_store.append(float(num_correct) / float(num_samples) * 100)\n",
    "    loss_store_val.append(valid_loss/len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ee98f5-5ec3-4463-a466-ceef407364ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_store_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4263/3817790461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_store_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_store_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_store_train1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Trainning loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_store_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_store_train' is not defined"
     ]
    }
   ],
   "source": [
    "loss_store_train=loss_store_train[-19:]\n",
    "plt.figure(1)\n",
    "plt.plot(loss_store_train1,label = 'Trainning loss')\n",
    "plt.plot(loss_store_val, label = 'Validation loss') \n",
    "plt.legend() \n",
    "plt.xlabel('Epoch') \n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss w.r.t epoch, ResNet18') \n",
    "plt.figure(2)\n",
    "plt.plot(acc_store,marker='.')\n",
    "plt.title('Validation accuracy w.r.t epoch, ResNet18') \n",
    "plt.xlabel('Epoch') \n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73c80b4c-8298-474f-9350-eac1a8346eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './resnet_try.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab5e1b8-b5c9-468d-8963-d3a049d90679",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4263/3230188255.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "model = model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ddb182e-6282-4128-983f-83805883363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-4, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8a66222-2d07-4b39-ab08-2a081b521298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.948\n",
      "Got 6221 / 10000 with accuracy 62.21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8917/4176438302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss_ep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss in epoch {epoch}: {loss_ep/len(train_dl):.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss_store_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_ep\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoches=20\n",
    "xaxis=np.arange(0,epoches)\n",
    "\n",
    "for epoch in range(epoches): #I decided to train the model for 50 epochs\n",
    "    loss_ep = 0\n",
    "    model.train()\n",
    "    for _, (data, targets) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        ## Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        score = model(data)\n",
    "        loss = criterion(score,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        loss_ep += loss.item()\n",
    "    print(f\"Loss in epoch {epoch}: {loss_ep/len(train_dl):.3f}\")\n",
    "    loss_store_train1.append(loss_ep/len(train_dl))\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        for _, (data,targets) in enumerate(val_dl):\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            ## Forward Pass\n",
    "            score = model(data)\n",
    "            _, predictions = score.max(1)\n",
    "            loss2 = criterion(score,targets)\n",
    "            valid_loss += loss2.item()\n",
    "            \n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}\"\n",
    "        )\n",
    "        acc_store.append(float(num_correct) / float(num_samples) * 100)\n",
    "    loss_store_val.append(valid_loss/len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0770ff09-22cc-44f5-b187-7b8566370bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m92",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m92"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
