{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conatminated.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvOS8gDhVB57",
        "outputId": "7168a62a-dc0e-4b58-8bcc-09f857346ac3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 30 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 40 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 71 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 102 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 112 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 122 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 133 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 143 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 153 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 163 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 174 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 184 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 194 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 204 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 215 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 225 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 235 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 245 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 256 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 266 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 276 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 286 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 296 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 307 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 317 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 327 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 337 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 348 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 358 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 368 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 378 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 389 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 399 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 409 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 419 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 430 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 440 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 450 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 460 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 471 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 481 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 491 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 501 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 512 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 522 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 532 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 542 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 552 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 563 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 573 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 583 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 593 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 604 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 614 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 624 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 634 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 645 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 655 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 665 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 675 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 686 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 696 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 706 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 716 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 719 kB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 346 kB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 61.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 197 kB 70.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 70.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 19.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 58.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.8 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5qfky9_0iNh",
        "outputId": "9322a515-c791-4066-b098-55a4043570bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (2.6.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (21.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied: fastcore<1.5,>=1.3.27 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.12.0+cu113)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.2)\n",
            "Requirement already satisfied: torch<1.12,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.11.0+cu113)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (6.0)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.0.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.21.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.64.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.9.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install fastai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "%matplotlib inline\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "\n",
        "is_cuda=torch.cuda.is_available()\n",
        "device=torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1PjTOqe-ZK5",
        "outputId": "e9d0fde4-dcc8-46b1-b80c-e7b2df1edac8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fastai\n",
        "from fastai.basics import *\n",
        "from fastai.vision.all import *\n",
        "from fastai.callback.all import *\n",
        "from fastai.distributed import *\n",
        "from fastprogress import fastprogress\n",
        "from torchvision.models import *\n",
        "from fastai.vision.models.xresnet import *\n",
        "from fastai.callback.mixup import *\n",
        "from fastcore.script import *\n",
        "from fastai.optimizer import OptimWrapper"
      ],
      "metadata": {
        "id": "8dOJ9ePo1RWP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.IMAGENETTE_160)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "id": "NpmIaBG_1S__",
        "outputId": "66a883ee-9b12-4606-ebac-444fb85c41ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='99008512' class='' max='99003388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.01% [99008512/99003388 00:02<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dl_fastai = ImageDataLoaders.from_folder(path, valid='val', \n",
        "    item_tfms=RandomResizedCrop(128, min_scale=0.35), batch_tfms=Normalize.from_stats(*imagenet_stats))\n",
        "train_dl_fastai = ImageDataLoaders.from_folder(path, train='train', \n",
        "    item_tfms=RandomResizedCrop(128, min_scale=0.35), batch_tfms=Normalize.from_stats(*imagenet_stats))"
      ],
      "metadata": {
        "id": "Ese-F6f01hMU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = get_image_files(path)\n",
        "dblock = DataBlock()\n",
        "dsets = dblock.datasets(fnames)\n",
        "dblock = DataBlock(get_items = get_image_files)\n",
        "\n",
        "lbl_dict = dict(\n",
        "    n01440764='tench',\n",
        "    n02102040='English springer',\n",
        "    n02979186='cassette player',\n",
        "    n03000684='chain saw',\n",
        "    n03028079='church',\n",
        "    n03394916='French horn',\n",
        "    n03417042='garbage truck',\n",
        "    n03425413='gas pump',\n",
        "    n03445777='golf ball',\n",
        "    n03888257='parachute'\n",
        ")\n",
        "\n",
        "def label_func(fname):\n",
        "    return lbl_dict[parent_label(fname)]\n",
        "\n",
        "imagenette = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
        "                       get_items = get_image_files,\n",
        "                       get_y = Pipeline([parent_label, lbl_dict.__getitem__]),\n",
        "                       splitter = GrandparentSplitter(valid_name='val'),\n",
        "                       item_tfms = RandomResizedCrop(128, min_scale=0.35),\n",
        "                       batch_tfms = Normalize.from_stats(*imagenet_stats))\n",
        "i_d = imagenette.datasets(path)\n",
        "\n",
        "\n",
        "i_d.train\n",
        "#dls = imagenette.dataloaders(path)\n",
        "#dls.show_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxGclZgs8IGv",
        "outputId": "d33f072d-33b3-45ef-915b-e1dda7db3dfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9469) [(PILImage mode=RGB size=213x160, TensorCategory(0)),(PILImage mode=RGB size=213x160, TensorCategory(0)),(PILImage mode=RGB size=213x160, TensorCategory(0)),(PILImage mode=RGB size=213x160, TensorCategory(0)),(PILImage mode=RGB size=160x232, TensorCategory(0)),(PILImage mode=RGB size=240x160, TensorCategory(0)),(PILImage mode=RGB size=213x160, TensorCategory(0)),(PILImage mode=RGB size=161x160, TensorCategory(0)),(PILImage mode=RGB size=240x160, TensorCategory(0)),(PILImage mode=RGB size=240x160, TensorCategory(0))...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Set first filter  \n",
        "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        \n",
        "        # He initialization: \n",
        "        nn.init.kaiming_uniform_(self.conv1_1.weight, mode='fan_in', nonlinearity='relu')\n",
        "        \n",
        "        self.conv1_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_uniform_(self.conv1_2.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Set second filter \n",
        "        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_uniform_(self.conv2_1.weight, mode='fan_in', nonlinearity='relu')\n",
        "        \n",
        "        self.conv2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_uniform_(self.conv2_2.weight, mode='fan_in', nonlinearity='relu')\n",
        "        \n",
        "        # Set third filter\n",
        "        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_uniform_(self.conv3_1.weight, mode='fan_in', nonlinearity='relu')\n",
        "        \n",
        "        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_uniform_(self.conv3_2.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        #nn.init.kaiming_uniform_(self.conv3_1.weight, mode='fan_in', nonlinearity='relu')\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        if block is 1:\n",
        "          self.fc1 = nn.Linear(8192, 120)\n",
        "          self.fc2 = nn.Linear(120, 10) # CHANGE \n",
        "          self.fc3 = nn.Linear(84, 10)\n",
        "        else:  \n",
        "          self.fc1 = nn.Linear(32768, 400)\n",
        "          nn.init.kaiming_uniform_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
        "        \n",
        "          self.fc2 = nn.Linear(400, 10) # CHANGE \n",
        "          self.fc3 = nn.Linear(400, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if block is 1:  \n",
        "          # 1 VGG block: \n",
        "          x = self.pool(F.relu(self.conv1_2(F.relu(self.conv1_1(x)))))\n",
        "          x = torch.flatten(x, 1)\n",
        "          x = F.relu(self.fc1(x))\n",
        "          x = self.fc2(x)\n",
        "        else:\n",
        "          # 3 VGG block \n",
        "          x = self.pool(F.relu(self.conv1_2(F.relu(self.conv1_1(x)))))\n",
        "          x = self.pool(F.relu(self.conv2_2(F.relu(self.conv2_1(x)))))\n",
        "          x = self.pool(F.relu(self.conv3_2(F.relu(self.conv3_1(x)))))\n",
        "          x = torch.flatten(x, 1)\n",
        "          x = F.relu(self.fc1(x))\n",
        "          x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "qnT9UXh_9H3j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#|export\n",
        "class BaseLoss():\n",
        "    \"Same as `loss_cls`, but flattens input and target.\"\n",
        "    activation=decodes=noops\n",
        "    def __init__(self, \n",
        "        loss_cls, # Uninitialized PyTorch-compatible loss\n",
        "        *args,\n",
        "        axis:int=-1, # Class axis\n",
        "        flatten:bool=True, # Flatten `inp` and `targ` before calculating loss\n",
        "        floatify:bool=False, # Convert `targ` to `float`\n",
        "        is_2d:bool=True, # Whether `flatten` keeps one or two channels when applied\n",
        "        **kwargs\n",
        "    ):\n",
        "        store_attr(\"axis,flatten,floatify,is_2d\")\n",
        "        self.func = loss_cls(*args,**kwargs)\n",
        "        functools.update_wrapper(self, self.func)\n",
        "\n",
        "    def __repr__(self) -> str: return f\"FlattenedLoss of {self.func}\"\n",
        "    \n",
        "    @property\n",
        "    def reduction(self) -> str: return self.func.reduction\n",
        "    \n",
        "    @reduction.setter\n",
        "    def reduction(self, v:str):\n",
        "        \"Sets the reduction style (typically 'mean', 'sum', or 'none')\" \n",
        "        self.func.reduction = v\n",
        "\n",
        "    def _contiguous(self, x:Tensor) -> TensorBase:\n",
        "        \"Move `self.axis` to the last dimension and ensure tensor is contigous for `Tensor` otherwise just return\"\n",
        "        return TensorBase(x.transpose(self.axis,-1).contiguous()) if isinstance(x,torch.Tensor) else x\n",
        "\n",
        "    def __call__(self, \n",
        "        inp:(Tensor,list), # Predictions from a `Learner`\n",
        "        targ:(Tensor,list), # Actual y label\n",
        "        **kwargs\n",
        "    ) -> TensorBase: # `loss_cls` calculated on `inp` and `targ`\n",
        "        inp,targ  = map(self._contiguous, (inp,targ))\n",
        "        if self.floatify and targ.dtype!=torch.float16: targ = targ.float()\n",
        "        if targ.dtype in [torch.int8, torch.int16, torch.int32]: targ = targ.long()\n",
        "        if self.flatten: inp = inp.view(-1,inp.shape[-1]) if self.is_2d else inp.view(-1)\n",
        "        return self.func.__call__(inp, targ.view(-1) if self.flatten else targ, **kwargs)\n",
        "    \n",
        "    def to(self, device:torch.device):\n",
        "        \"Move the loss function to a specified `device`\"\n",
        "        if isinstance(self.func, nn.Module): self.func.to(device)\n",
        "\n",
        "#|export\n",
        "@delegates()\n",
        "class CrossEntropyLossFlat(BaseLoss):\n",
        "    \"Same as `nn.CrossEntropyLoss`, but flattens input and target.\"\n",
        "    y_int = True # y interpolation\n",
        "    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n",
        "    def __init__(self, \n",
        "        *args, \n",
        "        axis:int=-1, # Class axis\n",
        "        **kwargs\n",
        "    ): \n",
        "        super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n",
        "    \n",
        "    def decodes(self, x:Tensor) -> Tensor:    \n",
        "        \"Converts model output to target format\"\n",
        "        return x.argmax(dim=self.axis)\n",
        "    \n",
        "    def activation(self, x:Tensor) -> Tensor: \n",
        "        \"`nn.CrossEntropyLoss`'s fused activation function applied to model output\"\n",
        "        return F.softmax(x, dim=self.axis)\n",
        "\n",
        "tst = CrossEntropyLossFlat()"
      ],
      "metadata": {
        "id": "R3rK0SLyh8sZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block=3\n",
        "model = VGG() #to compile the model\n",
        "model = model.to(device=device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.SGD(model.parameters(), lr= learning_rate,momentum=0.9) \n",
        "pytorch_adamw = partial(OptimWrapper, opt=optim.SGD(model.parameters(), lr= learning_rate,momentum=0.9) )\n",
        "opt = Optimizer(params, sgd_step, lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lbl_dict = dict(\n",
        "    n01440764='tench',\n",
        "    n02102040='English springer',\n",
        "    n02979186='cassette player',\n",
        "    n03000684='chain saw',\n",
        "    n03028079='church',\n",
        "    n03394916='French horn',\n",
        "    n03417042='garbage truck',\n",
        "    n03425413='gas pump',\n",
        "    n03445777='golf ball',\n",
        "    n03888257='parachute'\n",
        ")\n",
        "\n",
        "\n",
        "source = untar_data(URLs.IMAGENETTE_160)\n",
        "fnames = get_image_files(source)\n",
        "\n",
        "tfm = Pipeline([parent_label, lbl_dict.__getitem__, Categorize(vocab = lbl_dict.values())])\n",
        "splits = GrandparentSplitter(valid_name='val')(fnames)\n",
        "dsets = Datasets(fnames, [[PILImage.create], [parent_label, lbl_dict.__getitem__, Categorize]], splits=splits)\n",
        "item_tfms = [ToTensor, RandomResizedCrop(128, min_scale=0.35)]\n",
        "batch_tfms = [IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]\n",
        "dls = dsets.dataloaders(after_item=item_tfms, after_batch=batch_tfms, bs=64, num_workers=8)\n",
        "\n",
        "model = VGG() #to compile the model\n",
        "model = model.to(device=device) #to send the model for training on either cuda or cpu\n",
        "\n",
        "#tst_sgd = fastai.optimizer.OptimWrapper(model.parameters(), torch.optim.SGD, lr=1e-3, momentum=0.9, weight_decay=1e-2)\n",
        "pytorch_adamw = partial(fastai.optimizer.OptimWrapper, opt=torch.optim.SGD)\n",
        "\n",
        "learn = Learner(dls, model, metrics=accuracy,opt_func=pytorch_adamw, loss_func = tst)\n",
        "learn._set_device(device)\n",
        "\n",
        "learn.fit_one_cycle(50, 5e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JZ76erhNGmq7",
        "outputId": "cc3ba204-081b-40ce-b1cf-0c6dbcfae5f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='43' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      86.00% [43/50 22:00<03:34]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.884216</td>\n",
              "      <td>1.756474</td>\n",
              "      <td>0.398217</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.753635</td>\n",
              "      <td>1.644160</td>\n",
              "      <td>0.454268</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.587111</td>\n",
              "      <td>1.526043</td>\n",
              "      <td>0.507261</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.502750</td>\n",
              "      <td>1.355749</td>\n",
              "      <td>0.558217</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.345973</td>\n",
              "      <td>1.378126</td>\n",
              "      <td>0.550828</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.273336</td>\n",
              "      <td>1.222539</td>\n",
              "      <td>0.590828</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.213820</td>\n",
              "      <td>1.100863</td>\n",
              "      <td>0.631847</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.129219</td>\n",
              "      <td>1.065984</td>\n",
              "      <td>0.654013</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.058663</td>\n",
              "      <td>1.046811</td>\n",
              "      <td>0.656560</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.008756</td>\n",
              "      <td>0.994588</td>\n",
              "      <td>0.678981</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.974926</td>\n",
              "      <td>0.960032</td>\n",
              "      <td>0.691720</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.928478</td>\n",
              "      <td>0.975943</td>\n",
              "      <td>0.686369</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.877048</td>\n",
              "      <td>0.879553</td>\n",
              "      <td>0.718981</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.831550</td>\n",
              "      <td>0.877834</td>\n",
              "      <td>0.716688</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.765192</td>\n",
              "      <td>0.851980</td>\n",
              "      <td>0.726115</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.770268</td>\n",
              "      <td>0.926953</td>\n",
              "      <td>0.701911</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.735554</td>\n",
              "      <td>0.849347</td>\n",
              "      <td>0.730191</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.695240</td>\n",
              "      <td>0.830864</td>\n",
              "      <td>0.739108</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.642678</td>\n",
              "      <td>0.857588</td>\n",
              "      <td>0.728153</td>\n",
              "      <td>00:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.629312</td>\n",
              "      <td>0.864999</td>\n",
              "      <td>0.726879</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.607282</td>\n",
              "      <td>0.802864</td>\n",
              "      <td>0.748280</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.559906</td>\n",
              "      <td>0.863955</td>\n",
              "      <td>0.732739</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.557491</td>\n",
              "      <td>0.837938</td>\n",
              "      <td>0.739108</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.521424</td>\n",
              "      <td>0.816840</td>\n",
              "      <td>0.745732</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.515504</td>\n",
              "      <td>0.877520</td>\n",
              "      <td>0.728662</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.486108</td>\n",
              "      <td>0.907930</td>\n",
              "      <td>0.735032</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.488046</td>\n",
              "      <td>0.866604</td>\n",
              "      <td>0.742166</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.445351</td>\n",
              "      <td>0.909194</td>\n",
              "      <td>0.732229</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.415890</td>\n",
              "      <td>0.862261</td>\n",
              "      <td>0.748025</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.421951</td>\n",
              "      <td>0.811960</td>\n",
              "      <td>0.765096</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.376537</td>\n",
              "      <td>0.798524</td>\n",
              "      <td>0.771720</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.344061</td>\n",
              "      <td>0.833927</td>\n",
              "      <td>0.760764</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.356803</td>\n",
              "      <td>0.833494</td>\n",
              "      <td>0.762803</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.314903</td>\n",
              "      <td>0.846383</td>\n",
              "      <td>0.772484</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.315898</td>\n",
              "      <td>0.900745</td>\n",
              "      <td>0.756178</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.332553</td>\n",
              "      <td>0.801920</td>\n",
              "      <td>0.771975</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.285620</td>\n",
              "      <td>0.833523</td>\n",
              "      <td>0.776051</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.281472</td>\n",
              "      <td>0.855618</td>\n",
              "      <td>0.771975</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.256027</td>\n",
              "      <td>0.861769</td>\n",
              "      <td>0.769172</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.251416</td>\n",
              "      <td>0.812667</td>\n",
              "      <td>0.781911</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.226966</td>\n",
              "      <td>0.843994</td>\n",
              "      <td>0.782675</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.209130</td>\n",
              "      <td>0.852894</td>\n",
              "      <td>0.783949</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.197242</td>\n",
              "      <td>0.852315</td>\n",
              "      <td>0.785987</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='30' class='' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      20.41% [30/147 00:05<00:22 0.1845]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='49' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      98.00% [49/50 25:05<00:30]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.884216</td>\n",
              "      <td>1.756474</td>\n",
              "      <td>0.398217</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.753635</td>\n",
              "      <td>1.644160</td>\n",
              "      <td>0.454268</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.587111</td>\n",
              "      <td>1.526043</td>\n",
              "      <td>0.507261</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.502750</td>\n",
              "      <td>1.355749</td>\n",
              "      <td>0.558217</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.345973</td>\n",
              "      <td>1.378126</td>\n",
              "      <td>0.550828</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.273336</td>\n",
              "      <td>1.222539</td>\n",
              "      <td>0.590828</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.213820</td>\n",
              "      <td>1.100863</td>\n",
              "      <td>0.631847</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.129219</td>\n",
              "      <td>1.065984</td>\n",
              "      <td>0.654013</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.058663</td>\n",
              "      <td>1.046811</td>\n",
              "      <td>0.656560</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.008756</td>\n",
              "      <td>0.994588</td>\n",
              "      <td>0.678981</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.974926</td>\n",
              "      <td>0.960032</td>\n",
              "      <td>0.691720</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.928478</td>\n",
              "      <td>0.975943</td>\n",
              "      <td>0.686369</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.877048</td>\n",
              "      <td>0.879553</td>\n",
              "      <td>0.718981</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.831550</td>\n",
              "      <td>0.877834</td>\n",
              "      <td>0.716688</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.765192</td>\n",
              "      <td>0.851980</td>\n",
              "      <td>0.726115</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.770268</td>\n",
              "      <td>0.926953</td>\n",
              "      <td>0.701911</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.735554</td>\n",
              "      <td>0.849347</td>\n",
              "      <td>0.730191</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.695240</td>\n",
              "      <td>0.830864</td>\n",
              "      <td>0.739108</td>\n",
              "      <td>00:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.642678</td>\n",
              "      <td>0.857588</td>\n",
              "      <td>0.728153</td>\n",
              "      <td>00:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.629312</td>\n",
              "      <td>0.864999</td>\n",
              "      <td>0.726879</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.607282</td>\n",
              "      <td>0.802864</td>\n",
              "      <td>0.748280</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.559906</td>\n",
              "      <td>0.863955</td>\n",
              "      <td>0.732739</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.557491</td>\n",
              "      <td>0.837938</td>\n",
              "      <td>0.739108</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.521424</td>\n",
              "      <td>0.816840</td>\n",
              "      <td>0.745732</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.515504</td>\n",
              "      <td>0.877520</td>\n",
              "      <td>0.728662</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.486108</td>\n",
              "      <td>0.907930</td>\n",
              "      <td>0.735032</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.488046</td>\n",
              "      <td>0.866604</td>\n",
              "      <td>0.742166</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.445351</td>\n",
              "      <td>0.909194</td>\n",
              "      <td>0.732229</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.415890</td>\n",
              "      <td>0.862261</td>\n",
              "      <td>0.748025</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.421951</td>\n",
              "      <td>0.811960</td>\n",
              "      <td>0.765096</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.376537</td>\n",
              "      <td>0.798524</td>\n",
              "      <td>0.771720</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.344061</td>\n",
              "      <td>0.833927</td>\n",
              "      <td>0.760764</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.356803</td>\n",
              "      <td>0.833494</td>\n",
              "      <td>0.762803</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.314903</td>\n",
              "      <td>0.846383</td>\n",
              "      <td>0.772484</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.315898</td>\n",
              "      <td>0.900745</td>\n",
              "      <td>0.756178</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.332553</td>\n",
              "      <td>0.801920</td>\n",
              "      <td>0.771975</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.285620</td>\n",
              "      <td>0.833523</td>\n",
              "      <td>0.776051</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.281472</td>\n",
              "      <td>0.855618</td>\n",
              "      <td>0.771975</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.256027</td>\n",
              "      <td>0.861769</td>\n",
              "      <td>0.769172</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.251416</td>\n",
              "      <td>0.812667</td>\n",
              "      <td>0.781911</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.226966</td>\n",
              "      <td>0.843994</td>\n",
              "      <td>0.782675</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.209130</td>\n",
              "      <td>0.852894</td>\n",
              "      <td>0.783949</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.197242</td>\n",
              "      <td>0.852315</td>\n",
              "      <td>0.785987</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.170683</td>\n",
              "      <td>0.862985</td>\n",
              "      <td>0.787261</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.172249</td>\n",
              "      <td>0.865967</td>\n",
              "      <td>0.782930</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.164350</td>\n",
              "      <td>0.842859</td>\n",
              "      <td>0.786752</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.172823</td>\n",
              "      <td>0.851450</td>\n",
              "      <td>0.788790</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.162291</td>\n",
              "      <td>0.845873</td>\n",
              "      <td>0.786752</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.141564</td>\n",
              "      <td>0.850008</td>\n",
              "      <td>0.789554</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/62 00:00<00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}